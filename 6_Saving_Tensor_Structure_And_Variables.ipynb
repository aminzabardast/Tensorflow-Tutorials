{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_Saving_Tensor_Structure_And_Variables.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/aminzabardast/Tensorflow-Tutorials/blob/dev/6_Saving_Tensor_Structure_And_Variables.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "FG3MnAx8qZXq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Saving Tensor Structure and Variables"
      ]
    },
    {
      "metadata": {
        "id": "5bjw97YvXUJ9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Developed model in Notebook 5"
      ]
    },
    {
      "metadata": {
        "id": "S_383NUDqfyc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "f93341e1-9fc2-4e6d-80ef-cc481c235c71"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "\n",
        "\n",
        "def conv2d(input_data, kernel_shape, activation_function=tf.nn.relu, name=None):\n",
        "    weights = tf.Variable(tf.truncated_normal(kernel_shape, stddev=0.1))\n",
        "    biases = tf.Variable(tf.constant(.1, shape=[kernel_shape[3]]))\n",
        "    conv_result = tf.nn.conv2d(input_data, weights, strides=[1, 1, 1, 1], padding='SAME', name=name) + biases\n",
        "    return activation_function(conv_result)\n",
        "\n",
        "\n",
        "def maxPool2x2(input_data, name=None):\n",
        "    return tf.nn.max_pool(input_data, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
        "\n",
        "\n",
        "def fullyConnected(input_data, shape, activation_function=tf.nn.relu, name=None):\n",
        "    weights = tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
        "    biases = tf.Variable(tf.zeros([1, shape[1]]) + 0.1)\n",
        "    return activation_function(tf.matmul(input_data, weights) + biases, name=name)\n",
        "\n",
        "\n",
        "def computeAccuracy(v_xs, v_ys):\n",
        "    y_pre = sess.run(rfc2, feed_dict={xs: v_xs})\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys})\n",
        "    return result\n",
        "\n",
        "\n",
        "with tf.name_scope('Input'):\n",
        "    # Intensity values are between 0 and 255.\n",
        "    # This will be converted to between 0 and 1.\n",
        "    xs = tf.placeholder(dtype=tf.float32, shape=[None, 784], name='Data')/255\n",
        "    # MNIST data are storred in vectors and the need to be reshaped to 28*28 image.\n",
        "    x_images = tf.reshape(xs,shape=[-1, 28, 28, 1], name='ReshapedData')\n",
        "    \n",
        "with tf.name_scope('Truth'):\n",
        "    ys = tf.placeholder(dtype=tf.float32, shape=[None, 10], name='Truth')\n",
        "\n",
        "with tf.name_scope('Conv1'):\n",
        "    # 5x5 Kernel, Feature Map Input 1 (Gray scale image) and Output 32\n",
        "    kernel1_shape = [5, 5, 1, 32]\n",
        "    # Using kernel1 to calculate the convolved layer / Output size: 28 x 28 x 32\n",
        "    rconv1 = conv2d(x_images, kernel1_shape)\n",
        "\n",
        "with tf.name_scope('MaxPool1'):\n",
        "    # Max polling the result of Conv1 layer / Output size: 14 x 14 x 32\n",
        "    rpool1 = maxPool2x2(rconv1)\n",
        "\n",
        "with tf.name_scope('Conv2'):\n",
        "    # 3x3 Kernel, Feature Map Input 32 and Output 64\n",
        "    kernel2_shape = [3, 3, 32, 64]\n",
        "    # Using kernel2 to calculate the convolved layer / Output size: 14 x 14 x 64\n",
        "    rconv2 = conv2d(rpool1, kernel2_shape)\n",
        "\n",
        "with tf.name_scope('MaxPool2'):\n",
        "    # Max polling the result of Conv2 layer / Output size: 7 x 7 x 64\n",
        "    rpool2 = maxPool2x2(rconv2)\n",
        "\n",
        "with tf.name_scope('FC1'):\n",
        "    # Shape of the layer\n",
        "    fc1_shape = [7*7*64, 1024]\n",
        "    # Flattening From [n,7,7,64] to [n,3136]\n",
        "    rpool2_flattened = tf.reshape(rpool2, shape=[-1, 7*7*64])\n",
        "    rfc1 = fullyConnected(rpool2_flattened, fc1_shape)\n",
        "\n",
        "with tf.name_scope('FC2'):\n",
        "    # Shape of the layer\n",
        "    fc2_shape = [1024, 10]\n",
        "    rfc2 = fullyConnected(rfc1, fc2_shape, tf.nn.softmax, name='prediction')\n",
        "\n",
        "with tf.name_scope('Loss'):\n",
        "    # Cross Entropy as the loss function\n",
        "    crossEntropy = tf.reduce_mean(-tf.reduce_sum(ys*tf.log(rfc2), reduction_indices=[1]))\n",
        "    tf.summary.scalar(name='CrossEntropy', tensor=crossEntropy)\n",
        "\n",
        "with tf.name_scope('Optimizer'):\n",
        "    # Optmizing using Simple Gradient Descent\n",
        "    trainStep = tf.train.GradientDescentOptimizer(learning_rate=0.01, name='train_step').minimize(crossEntropy)\n",
        "\n",
        "# Creating Session\n",
        "sess = tf.Session()\n",
        "\n",
        "# Initiating Variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Merging all summaries\n",
        "merged = tf.summary.merge_all()\n",
        "\n",
        "# Creating File Writers\n",
        "trainWriter = tf.summary.FileWriter(logdir='logs/train', graph=sess.graph)\n",
        "testWriter = tf.summary.FileWriter(logdir='logs/test', graph=sess.graph)\n",
        "\n",
        "# Initiating Session\n",
        "sess.run(init)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-325498c9522f>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UQ9IwbSSdLne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training and Saving"
      ]
    },
    {
      "metadata": {
        "id": "hRygloeQdKNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "52be1132-c356-4fc2-efa4-995e8859be68"
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver(max_to_keep=4, keep_checkpoint_every_n_hours=2)\n",
        "\n",
        "# Training\n",
        "for epoch in range(100):\n",
        "    # Training with some Images in one epoch\n",
        "    trainBatchXs, trainBatchYs = mnist.train.next_batch(300)\n",
        "    testBatchXs, testBatchYs = mnist.test.next_batch(300)\n",
        "    \n",
        "    # Forward and backward pass\n",
        "    sess.run(trainStep, feed_dict={xs: trainBatchXs, ys: trainBatchYs})\n",
        "    \n",
        "    # Adding the state of network to logs\n",
        "    trainWriter.add_summary(sess.run(merged, feed_dict={xs: trainBatchXs, ys: trainBatchYs}), epoch)\n",
        "    testWriter.add_summary(sess.run(merged, feed_dict={xs: testBatchXs, ys: testBatchYs}), epoch)\n",
        "    \n",
        "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
        "        print('Epoch: {}'.format(epoch+1))\n",
        "    \n",
        "    if (epoch+1) % 10 == 0:\n",
        "        saver.save(sess, 'saved-network/model', global_step=epoch+1)\n",
        "\n",
        "# Calculating final accuracy\n",
        "accuracy = computeAccuracy(mnist.test.images, mnist.test.labels)*100\n",
        "print('\\nFinal Accuracy: %{0:.2f}'.format(accuracy))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Epoch: 10\n",
            "Epoch: 20\n",
            "Epoch: 30\n",
            "Epoch: 40\n",
            "Epoch: 50\n",
            "Epoch: 60\n",
            "Epoch: 70\n",
            "Epoch: 80\n",
            "Epoch: 90\n",
            "Epoch: 100\n",
            "\n",
            "Final Accuracy: %88.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mhpI0J6swIaD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "9fb73833-8462-4d36-b4c1-ffa350486227"
      },
      "cell_type": "code",
      "source": [
        "!tar czvf saved.tar.gz saved-network\n",
        "\n",
        "from google.colab import files\n",
        "files.download('saved.tar.gz')\n",
        "\n",
        "!rm -rvf logs* saved*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saved-network/\r\n",
            "saved-network/model-80.data-00000-of-00001\n",
            "saved-network/model-100.index\n",
            "saved-network/model-100.meta\n",
            "saved-network/model-90.index\n",
            "saved-network/model-70.meta\n",
            "saved-network/model-90.data-00000-of-00001\n",
            "saved-network/model-100.data-00000-of-00001\n",
            "saved-network/model-70.data-00000-of-00001\n",
            "saved-network/model-80.index\n",
            "saved-network/model-90.meta\n",
            "saved-network/checkpoint\n",
            "saved-network/model-70.index\n",
            "saved-network/model-80.meta\n",
            "removed 'logs/train/events.out.tfevents.1533506288.18e7c463b108'\n",
            "removed directory 'logs/train'\n",
            "removed 'logs/test/events.out.tfevents.1533506288.18e7c463b108'\n",
            "removed directory 'logs/test'\n",
            "removed directory 'logs'\n",
            "removed 'saved-network/model-80.data-00000-of-00001'\n",
            "removed 'saved-network/model-100.index'\n",
            "removed 'saved-network/model-100.meta'\n",
            "removed 'saved-network/model-90.index'\n",
            "removed 'saved-network/model-70.meta'\n",
            "removed 'saved-network/model-90.data-00000-of-00001'\n",
            "removed 'saved-network/model-100.data-00000-of-00001'\n",
            "removed 'saved-network/model-70.data-00000-of-00001'\n",
            "removed 'saved-network/model-80.index'\n",
            "removed 'saved-network/model-90.meta'\n",
            "removed 'saved-network/checkpoint'\n",
            "removed 'saved-network/model-70.index'\n",
            "removed 'saved-network/model-80.meta'\n",
            "removed directory 'saved-network'\n",
            "removed 'saved.tar.gz'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_Convolutional_Neural_Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/aminzabardast/Tensorflow-Tutorials/blob/dev/5_Convolutional_Neural_Network.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "R4-KW_AEY9a0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks With Tensor Flow"
      ]
    },
    {
      "metadata": {
        "id": "xeC7mIJo8qjG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Convolutional Neural Networks**, also known as **ConvNet**s or **CNN**s, are a type of Neural Networks that performe better in specific data types. These type of networks are specially better for image analysis.\n",
        "\n",
        "This notebook is dedicated to creating CNNs using Tensoflow library and the specific example that will be studied is [MNIST database](http://yann.lecun.com/exdb/mnist/) for handwritten digit recognition."
      ]
    },
    {
      "metadata": {
        "id": "fRi11Zs_B60o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading Libraries and Data"
      ]
    },
    {
      "metadata": {
        "id": "DyXHPyHRdJE5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The only package needed for this example is `Tensorflow`.  `MNIST` data is available as a tutorial example inside tensorflow package. "
      ]
    },
    {
      "metadata": {
        "id": "5CwoR4hTB9yI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7395caeb-f17d-45a0-fff2-1c482d58e3f3"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HLKqv0EcdsbJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "By calling `input_data.read_data_sets('MNIST_data', one_hot=True)`, Tensorflow will download the data for you."
      ]
    },
    {
      "metadata": {
        "id": "2nju2b4bCGXF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "f257e9c4-308e-4ab4-dd95-d8302f4c20f9"
      },
      "cell_type": "code",
      "source": [
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-71e12f4bac70>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FrnfcdKyB14M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Function: Divide and Conquer"
      ]
    },
    {
      "metadata": {
        "id": "Ps6-Uoo0fPse",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is very good practice to approach your problem by dividing and conquering. Here, the problem at hand becomes easier by encapsulating each logical part into a function.\n",
        "\n",
        "Creating Concolution, Max Pooling, and Fully Connected layers are handled by `conv2d`, `maxPool2x2`, and `fullyConnected` respectively. \n",
        "\n",
        "Accuracy of the mthod is calculated by `computeAccuracy`. To do this all of the test images will run through the network and the percentage of true results will be calcularted."
      ]
    },
    {
      "metadata": {
        "id": "KIfrFdVYB078",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "e0ec3bcc-7cec-4070-fccd-49f05c77c50f"
      },
      "cell_type": "code",
      "source": [
        "def conv2d(input_data, kernel_shape, activation_function=tf.nn.relu, name=None):\n",
        "    weights = tf.Variable(tf.truncated_normal(kernel_shape, stddev=0.1))\n",
        "    biases = tf.Variable(tf.constant(.1, shape=[kernel_shape[3]]))\n",
        "    conv_result = tf.nn.conv2d(input_data, weights, strides=[1, 1, 1, 1], padding='SAME', name=name) + biases\n",
        "    return activation_function(conv_result)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ccRTnX_3KLoO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "da3e3275-6399-44d9-c764-10dbf62aac25"
      },
      "cell_type": "code",
      "source": [
        "def maxPool2x2(input_data, name=None):\n",
        "    return tf.nn.max_pool(input_data, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Lbbn1wbLm_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9a5abca4-0762-4fae-ac41-ce848127c9dc"
      },
      "cell_type": "code",
      "source": [
        "def fullyConnected(input_data, shape, activation_function=tf.nn.relu, name=None):\n",
        "    weights = tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
        "    biases = tf.Variable(tf.zeros([1, shape[1]]) + 0.1)\n",
        "    return activation_function(tf.matmul(input_data, weights) + biases)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HL5n5Rs9PF6m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f389d1cd-2366-4b40-f05e-be94d41fdc1a"
      },
      "cell_type": "code",
      "source": [
        "def computeAccuracy(v_xs, v_ys):\n",
        "    y_pre = sess.run(rfc2, feed_dict={xs: v_xs})\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys})\n",
        "    return result"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0U88G4EjV7tt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating The Network and Training"
      ]
    },
    {
      "metadata": {
        "id": "EfRLLwCMqkLS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The target structure in this example is not considered complicated. This is a simple CNN with two **Convolutional** layers, each followed by a **2 by 2 Max Pooling** layer. Then the result of layer will be flattened and go through Two **Fully Connected** layers. The power of CNN in Image Analysis is apparent here since this simple network can achieve $\\%98$ accuracy in **MNIST** data set with simple **Stochastic Gradient Descent**.\n",
        "\n",
        "![Graphical Representation Of Our Network](https://raw.githubusercontent.com/aminzabardast/Tensorflow-Tutorials/dev/figures/5_CNN_structure.png)\n",
        "\n",
        "Images in **MNIST** datasset are all cropped to **28 by 28** size, which is the size of input layer. The input will be convolved by a **5 by 5** layer and takes the single chanel image to a 32 changel image. This means the input to layer two is a **28 by 28 by 32** image. This result will go through a 2 by 2 Max Pooling and becomes a **14 by 14 by 32** image. \n",
        "\n",
        "This **14 by 14 by 32** image will go through another convolution with **3 by 3 convolving kernel**. This results in a **14 by 14 by 64** image which will be converted to a **7 by 7 by 64** image after max pooling.\n",
        "\n",
        "The intention behind this approach is to encode spacial data of the image into a smaller space with more channels. The information might seem scrambled afterwards but in reality this extracts the essence of the image so classification step can happen.\n",
        "\n",
        "Additionally, the convolution operation makes the network invariant to spatial changes in the image. This means the network will not be mistaken if the digit is not centered exactly in the center, hence this is a spatial invarient approach.\n",
        "\n",
        "Finally, **7 by 7 by 64** output will beflattened into a **3136** ($7 \\times 7 \\times 64$) array. This array goes through two fully connected network to be classified."
      ]
    },
    {
      "metadata": {
        "id": "FVuRiGmqnyxv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "5a7bd0ac-cda2-4d3c-97be-8e10536ce987"
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('Input'):\n",
        "    # Intensity values are between 0 and 255.\n",
        "    # This will be converted to between 0 and 1.\n",
        "    xs = tf.placeholder(dtype=tf.float32, shape=[None, 784], name='Data')/255\n",
        "    # MNIST data are storred in vectors and the need to be reshaped to 28*28 image.\n",
        "    x_images = tf.reshape(xs,shape=[-1, 28, 28, 1], name='ReshapedData')\n",
        "    \n",
        "with tf.name_scope('Truth'):\n",
        "    ys = tf.placeholder(dtype=tf.float32, shape=[None, 10], name='Truth')\n",
        "\n",
        "with tf.name_scope('Conv1'):\n",
        "    # 5x5 Kernel, Feature Map Input 1 (Gray scale image) and Output 32\n",
        "    kernel1_shape = [5, 5, 1, 32]\n",
        "    # Using kernel1 to calculate the convolved layer / Output size: 28 x 28 x 32\n",
        "    rconv1 = conv2d(x_images, kernel1_shape)\n",
        "\n",
        "with tf.name_scope('MaxPool1'):\n",
        "    # Max polling the result of Conv1 layer / Output size: 14 x 14 x 32\n",
        "    rpool1 = maxPool2x2(rconv1)\n",
        "\n",
        "with tf.name_scope('Conv2'):\n",
        "    # 3x3 Kernel, Feature Map Input 32 and Output 64\n",
        "    kernel2_shape = [3, 3, 32, 64]\n",
        "    # Using kernel2 to calculate the convolved layer / Output size: 14 x 14 x 64\n",
        "    rconv2 = conv2d(rpool1, kernel2_shape)\n",
        "\n",
        "with tf.name_scope('MaxPool2'):\n",
        "    # Max polling the result of Conv2 layer / Output size: 7 x 7 x 64\n",
        "    rpool2 = maxPool2x2(rconv2)\n",
        "\n",
        "with tf.name_scope('FC1'):\n",
        "    # Shape of the layer\n",
        "    fc1_shape = [7*7*64, 1024]\n",
        "    # Flattening From [n,7,7,64] to [n,3136]\n",
        "    rpool2_flattened = tf.reshape(rpool2, shape=[-1, 7*7*64])\n",
        "    rfc1 = fullyConnected(rpool2_flattened, fc1_shape)\n",
        "\n",
        "with tf.name_scope('FC2'):\n",
        "    # Shape of the layer\n",
        "    fc2_shape = [1024, 10]\n",
        "    rfc2 = fullyConnected(rfc1, fc2_shape, tf.nn.softmax)\n",
        "\n",
        "with tf.name_scope('Loss'):\n",
        "    # Cross Entropy as the loss function\n",
        "    crossEntropy = tf.reduce_mean(-tf.reduce_sum(ys*tf.log(rfc2), reduction_indices=[1]))\n",
        "    tf.summary.scalar(name='CrossEntropy', tensor=crossEntropy)\n",
        "\n",
        "with tf.name_scope('Optimizer'):\n",
        "    # Optmizing using Simple Gradient Descent\n",
        "    trainStep = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(crossEntropy)\n",
        "\n",
        "# Creating Session\n",
        "sess = tf.Session()\n",
        "\n",
        "# Initiating Variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Merging all summaries\n",
        "merged = tf.summary.merge_all()\n",
        "\n",
        "# Creating File Writers\n",
        "trainWriter = tf.summary.FileWriter(logdir='logs/train', graph=sess.graph)\n",
        "testWriter = tf.summary.FileWriter(logdir='logs/test', graph=sess.graph)\n",
        "\n",
        "# Initiating Session\n",
        "sess.run(init)\n",
        "\n",
        "# Training\n",
        "for epoch in range(4000):\n",
        "    # Training with 100 Images in one epoch\n",
        "    trainBatchXs, trainBatchYs = mnist.train.next_batch(100)\n",
        "    testBatchXs, testBatchYs = mnist.test.next_batch(100)\n",
        "    \n",
        "    # Forward and backward pass\n",
        "    sess.run(trainStep, feed_dict={xs: trainBatchXs, ys: trainBatchYs})\n",
        "    \n",
        "    # Adding the state of network to logs\n",
        "    trainWriter.add_summary(sess.run(merged, feed_dict={xs: trainBatchXs, ys: trainBatchYs}), epoch)\n",
        "    testWriter.add_summary(sess.run(merged, feed_dict={xs: testBatchXs, ys: testBatchYs}), epoch)\n",
        "    \n",
        "    if (epoch+1) % 100 == 0 or epoch == 0:\n",
        "        print('Epoch: {}'.format(epoch+1))\n",
        "\n",
        "# Calculating final accuracy\n",
        "accuracy = computeAccuracy(mnist.test.images, mnist.test.labels)*100\n",
        "print('\\nFinal Accuracy: %{0:.2f}'.format(accuracy))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Epoch: 100\n",
            "Epoch: 200\n",
            "Epoch: 300\n",
            "Epoch: 400\n",
            "Epoch: 500\n",
            "Epoch: 600\n",
            "Epoch: 700\n",
            "Epoch: 800\n",
            "Epoch: 900\n",
            "Epoch: 1000\n",
            "Epoch: 1100\n",
            "Epoch: 1200\n",
            "Epoch: 1300\n",
            "Epoch: 1400\n",
            "Epoch: 1500\n",
            "Epoch: 1600\n",
            "Epoch: 1700\n",
            "Epoch: 1800\n",
            "Epoch: 1900\n",
            "Epoch: 2000\n",
            "Epoch: 2100\n",
            "Epoch: 2200\n",
            "Epoch: 2300\n",
            "Epoch: 2400\n",
            "Epoch: 2500\n",
            "Epoch: 2600\n",
            "Epoch: 2700\n",
            "Epoch: 2800\n",
            "Epoch: 2900\n",
            "Epoch: 3000\n",
            "Epoch: 3100\n",
            "Epoch: 3200\n",
            "Epoch: 3300\n",
            "Epoch: 3400\n",
            "Epoch: 3500\n",
            "Epoch: 3600\n",
            "Epoch: 3700\n",
            "Epoch: 3800\n",
            "Epoch: 3900\n",
            "Epoch: 4000\n",
            "\n",
            "Final Accuracy: %98.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_MIpvDwoV_kb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Downloading Log"
      ]
    },
    {
      "metadata": {
        "id": "_4hxErIooVAm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "735176a7-3ff0-4827-e4a0-5dd4df084453"
      },
      "cell_type": "code",
      "source": [
        "!tar czvf logs.tar.gz logs\n",
        "\n",
        "from google.colab import files\n",
        "files.download('logs.tar.gz')\n",
        "\n",
        "!rm -rvf logs*"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logs/\r\n",
            "logs/train/\r\n",
            "logs/train/events.out.tfevents.1530643617.fd4b4cae378d\r\n",
            "logs/test/\r\n",
            "logs/test/events.out.tfevents.1530643617.fd4b4cae378d\n",
            "removed 'logs/train/events.out.tfevents.1530643617.fd4b4cae378d'\n",
            "removed directory 'logs/train'\n",
            "removed 'logs/test/events.out.tfevents.1530643617.fd4b4cae378d'\n",
            "removed directory 'logs/test'\n",
            "removed directory 'logs'\n",
            "removed 'logs.tar.gz'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
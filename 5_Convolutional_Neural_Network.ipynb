{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_Convolutional_Neural_Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/aminzabardast/Tensorflow-Tutorials/blob/dev/5_Convolutional_Neural_Network.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "R4-KW_AEY9a0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks With Tensor Flow"
      ]
    },
    {
      "metadata": {
        "id": "xeC7mIJo8qjG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Convolutional Neural Networks**, also known as **ConvNet**s or **CNN**s, are a type of Neural Networks that performe better in specific data types. These type of networks are specially better for image analysis.\n",
        "\n",
        "This notebook is dedicated to creating CNNs using Tensoflow library and the specific example that will be studied is [MNIST database](http://yann.lecun.com/exdb/mnist/) for handwritten digit recognition."
      ]
    },
    {
      "metadata": {
        "id": "SiobR2APnwcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "690616c1-4b3d-44cb-915d-eea9bee9f022"
      },
      "cell_type": "code",
      "source": [
        "!pip show tensorflow\n",
        "!rm -rf logs"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\r\n",
            "Version: 1.9.0rc1\r\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\r\n",
            "Home-page: https://www.tensorflow.org/\r\n",
            "Author: Google Inc.\r\n",
            "Author-email: opensource@google.com\r\n",
            "License: Apache 2.0\r\n",
            "Location: /usr/local/lib/python3.6/dist-packages\r\n",
            "Requires: gast, six, setuptools, grpcio, wheel, termcolor, protobuf, absl-py, tensorboard, numpy, astor\r\n",
            "Required-by: \r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fRi11Zs_B60o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading Libraries and Data"
      ]
    },
    {
      "metadata": {
        "id": "5CwoR4hTB9yI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "46cc5da3-60d3-45aa-c4a7-76bd44fb7983"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2nju2b4bCGXF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "cc4edb01-3660-4ce1-8917-4d7426e2f427"
      },
      "cell_type": "code",
      "source": [
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-71e12f4bac70>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FrnfcdKyB14M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Function: Divide and Conquer"
      ]
    },
    {
      "metadata": {
        "id": "KIfrFdVYB078",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d07f6a0b-be1b-4b06-f57d-f30bfd49640d"
      },
      "cell_type": "code",
      "source": [
        "def conv2d(input_data, kernel_shape, activation_function=tf.nn.relu, name=None):\n",
        "    weights = tf.Variable(tf.truncated_normal(kernel_shape, stddev=0.1))\n",
        "    biases = tf.Variable(tf.constant(.1, shape=[kernel_shape[3]]))\n",
        "    conv_result = tf.nn.conv2d(input_data, weights, strides=[1,1,1,1], padding='SAME', name=name) + biases\n",
        "    return activation_function(conv_result)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ccRTnX_3KLoO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "06c16f7d-aec5-4e53-898d-4baac1e4162d"
      },
      "cell_type": "code",
      "source": [
        "def maxPool2x2(input_data, name=None):\n",
        "    return tf.nn.max_pool(input_data, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Lbbn1wbLm_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "61dc29f8-edba-4bd1-cc0f-a954828fe988"
      },
      "cell_type": "code",
      "source": [
        "def fullyConnectedLayer(input_data, shape, activation_function=tf.nn.relu, name=None):\n",
        "    weights = tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
        "    biases = tf.Variable(tf.zeros([1, shape[1]]) + 0.1)\n",
        "    return activation_function(tf.matmul(input_data, weights) + biases)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HL5n5Rs9PF6m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "92901922-9a6d-4276-ae88-74e3c9ea884d"
      },
      "cell_type": "code",
      "source": [
        "def compute_accuracy(v_xs, v_ys):\n",
        "    y_pre = sess.run(rfc2, feed_dict={xs: v_xs})\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys})\n",
        "    return result"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0U88G4EjV7tt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating The Network and Training"
      ]
    },
    {
      "metadata": {
        "id": "FVuRiGmqnyxv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "8d4a50da-24d2-4981-de9d-906dd7a8ce3a"
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('Input'):\n",
        "    xs = tf.placeholder(dtype=tf.float32, shape=[None, 784], name='Data')/255\n",
        "    x_images = tf.reshape(xs,shape=[-1, 28, 28, 1], name='ReshapedData')\n",
        "with tf.name_scope('Truth'):\n",
        "    ys = tf.placeholder(dtype=tf.float32, shape=[None, 10], name='Truth')\n",
        "\n",
        "with tf.name_scope('Conv1'):\n",
        "    kernel1_shape = [5, 5, 1, 32]  # 5x5 Kernel, Feature Map Input 1 (Gray scale image) and Output 32\n",
        "    rconv1 = conv2d(x_images, kernel1_shape)  # Using kernel1 to calculate the convolved layer / Output size: 28 x 28 x 32\n",
        "\n",
        "with tf.name_scope('MaxPool1'):\n",
        "    rpool1 = maxPool2x2(rconv1)  # Max polling the result of Conv1 layer / Output size: 14 x 14 x 32\n",
        "\n",
        "with tf.name_scope('Conv2'):\n",
        "    kernel2_shape = [3, 3, 32, 64]  # 3x3 Kernel, Feature Map Input 32 and Output 64\n",
        "    rconv2 = conv2d(rpool1, kernel2_shape)  # Using kernel2 to calculate the convolved layer / Output size: 14 x 14 x 64\n",
        "\n",
        "with tf.name_scope('MaxPool2'):\n",
        "    rpool2 = maxPool2x2(rconv2)  # Max polling the result of Conv2 layer / Output size: 7 x 7 x 64\n",
        "\n",
        "with tf.name_scope('FC1'):\n",
        "    fc1_shape = [7*7*64, 1024]  # Shape of the layer\n",
        "    rpool2_flattened = tf.reshape(rpool2, shape=[-1, 7*7*64])  # From [n,7,7,64] to [n,3136]\n",
        "    rfc1 = fullyConnectedLayer(rpool2_flattened, fc1_shape)\n",
        "\n",
        "with tf.name_scope('FC2'):\n",
        "    fc2_shape = [1024, 10]  # Shape of th layer\n",
        "    rfc2 = fullyConnectedLayer(rfc1, fc2_shape, tf.nn.softmax)\n",
        "\n",
        "with tf.name_scope('Loss'):\n",
        "    crossEntropy = tf.reduce_mean(-tf.reduce_sum(ys*tf.log(rfc2), reduction_indices=[1]))\n",
        "    tf.summary.scalar(name='CrossEntropy', tensor=crossEntropy)\n",
        "\n",
        "with tf.name_scope('Optimizer'):\n",
        "    trainStep = tf.train.AdamOptimizer().minimize(crossEntropy)\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "merged = tf.summary.merge_all()\n",
        "\n",
        "trainWriter = tf.summary.FileWriter(logdir='logs/train', graph=sess.graph)\n",
        "testWriter = tf.summary.FileWriter(logdir='logs/test', graph=sess.graph)\n",
        "\n",
        "sess.run(init)\n",
        "\n",
        "for epoch in range(2000):\n",
        "    trainBatchXs, trainBatchYs = mnist.train.next_batch(100)\n",
        "    testBatchXs, testBatchYs = mnist.test.next_batch(100)\n",
        "    \n",
        "    sess.run(trainStep, feed_dict={xs: trainBatchXs, ys: trainBatchYs})\n",
        "    \n",
        "    trainWriter.add_summary(sess.run(merged, feed_dict={xs: trainBatchXs, ys: trainBatchYs}), epoch)\n",
        "    testWriter.add_summary(sess.run(merged, feed_dict={xs: testBatchXs, ys: testBatchYs}), epoch)\n",
        "    \n",
        "    if (epoch+1) % 100 == 0 or epoch == 0:\n",
        "        print('Epoch: {}'.format(epoch+1))\n",
        "\n",
        "accuracy = compute_accuracy(mnist.test.images, mnist.test.labels)*100\n",
        "print('\\nFinal Accuracy: %{0:.2f}'.format(accuracy))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Epoch: 100\n",
            "Epoch: 200\n",
            "Epoch: 300\n",
            "Epoch: 400\n",
            "Epoch: 500\n",
            "Epoch: 600\n",
            "Epoch: 700\n",
            "Epoch: 800\n",
            "Epoch: 900\n",
            "Epoch: 1000\n",
            "Epoch: 1100\n",
            "Epoch: 1200\n",
            "Epoch: 1300\n",
            "Epoch: 1400\n",
            "Epoch: 1500\n",
            "Epoch: 1600\n",
            "Epoch: 1700\n",
            "Epoch: 1800\n",
            "Epoch: 1900\n",
            "Epoch: 2000\n",
            "\n",
            "Final Accuracy: %98.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_MIpvDwoV_kb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Downloading Log"
      ]
    },
    {
      "metadata": {
        "id": "_4hxErIooVAm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8751d6dd-4808-469c-f0c3-ce0df1db0401"
      },
      "cell_type": "code",
      "source": [
        "!tar czvf logs.tar.gz logs\n",
        "\n",
        "from google.colab import files\n",
        "files.download('logs.tar.gz')\n",
        "\n",
        "!rm -rvf logs*"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logs/\r\n",
            "logs/train/\r\n",
            "logs/train/events.out.tfevents.1530557341.792ba13cd039\r\n",
            "logs/test/\r\n",
            "logs/test/events.out.tfevents.1530557341.792ba13cd039\n",
            "removed 'logs/train/events.out.tfevents.1530557341.792ba13cd039'\n",
            "removed directory 'logs/train'\n",
            "removed 'logs/test/events.out.tfevents.1530557341.792ba13cd039'\n",
            "removed directory 'logs/test'\n",
            "removed directory 'logs'\n",
            "removed 'logs.tar.gz'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}